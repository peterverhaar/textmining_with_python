{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename\n",
    "my_text = 'https://github.com/peterverhaar/textmining_with_python/blob/main/Corpus/BraveNewWorld.txt'\n",
    "file_name = basename(my_text)\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "import sys\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "import requests\n",
    "\n",
    "\n",
    "def download(url):\n",
    "    response = requests.get(url)\n",
    "    if response:\n",
    "        new_file_name = basename(url)\n",
    "        print(f\"{new_file_name} is downloaded!\")\n",
    "        out = open(new_file_name,'w',encoding='utf-8')\n",
    "        out.write(response.text)\n",
    "        out.close()\n",
    "        \n",
    "def sorted_by_value( dict , ascending = True ):\n",
    "    if ascending: \n",
    "        return {k: v for k, v in sorted(dict.items(), key=lambda item: item[1])}\n",
    "    else:\n",
    "        return {k: v for k, v in reversed( sorted(dict.items(), key=lambda item: item[1]))}\n",
    "\n",
    "download('https://raw.githubusercontent.com/peterverhaar/textmining_with_python/refs/heads/main/text_mining.py')\n",
    "download(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Lexicons\n",
    "\n",
    "Lists of frequent words can help us to develop an rough understanding of the main concerns of the text. They don't necessary offer insights on the broader topics or the themes that are discussed within the text. If we want to investigate the semantics of the text at a somehwat deeper level, it can be useful to make use of word lists which map the words that occur in a text to broader pre-defined semantic categories. Such lists of words are often referred to a 'lexicons'. We can make a lexicon listing words related to 'religion', for instance. Next, by counting the numbers of times a text uses one of the words on the lexicon, we can form an impression of the importance of this particular theme within this text.  \n",
    "\n",
    "Examples of applications which have implemented this type of 'semantic tagging' include\n",
    "the [Harvard General Inquirer (HGI)](http://www.wjh.harvard.edu/~inquirer/homecat.htm), [the Linguistic Inquiry and Word Count (LIWC)\n",
    "tool](http://liwc.wpengine.com/)  and the [UCREL Semantic Analysis System (USAS)](http://ucrel.lancs.ac.uk/usas/). The programmers responsible for the *Harvard General Inquirer*, for example, have defined 182 semantic categories, and they have compiled long list of words pertaining to these categories.  \n",
    "\n",
    "To let you work with the possibilities of semantic tagging, a number of the lexicons that have been made available have been downloaded and merged. Next to the lexicons developed for the HGI and USAS, the word lists created for this course also include terms taken from lists compiled by [Bing Liu](https://www.cs.uic.edu/~liub/) and by the project team that worked on the [Multi-Perspective Question Answering (MPQA) tool](http://mpqa.cs.pitt.edu/). \n",
    "\n",
    "The merged semantic lexicons can be found here: \n",
    "https://github.com/peterverhaar/semantic-tagging/tree/main/Lexicons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, the lexicon files that are available are all mentioned in the list named `lexicon_files`. The code downloads all of these lexicon files, and saved these in a dictionary named `lexicons`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import os\n",
    "\n",
    "baseUrl = 'https://raw.githubusercontent.com/peterverhaar/semantic-tagging/main/Lexicons/'\n",
    "lexicon_files = [  'Academic.txt' , 'Economics.txt' ,  'Legal.txt' , 'Military.txt' , 'Movement.txt' , 'Pain.txt' , 'Passive.txt' , 'Pleasure.txt' , 'Politics.txt' , 'Power.txt' , 'Religion.txt' , 'Space.txt' , 'Time.txt' , 'Transportation.txt' , 'Vice.txt' , 'Weather.txt' , 'workAndEmployment.txt' ]\n",
    "\n",
    "dir = 'Lexicons'\n",
    "if not os.path.isdir(dir):\n",
    "    os.mkdir(dir)\n",
    "\n",
    "\n",
    "for l in lexicon_files:\n",
    "    topic = l[ : l.rindex('.') ]\n",
    "    response = requests.get( baseUrl + l)\n",
    "    words = []\n",
    "    if response:\n",
    "        response.encoding = 'utf-8'\n",
    "        out = open( os.path.join( dir , l ) , 'w' , encoding = 'utf-8' )\n",
    "        out.write( response.text )\n",
    "        out.close()\n",
    "\n",
    "print('Lexicons have been downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also work with create your own lexicon files. A lexicon file is simple a text file listing all the terms that are relevant. You can create new files, and add these to the directory `Lexicon`. You can also edit existing lexicons, or remove those lexicons that are not relevant to you. \n",
    "\n",
    "The following code reads in all the lexicon files from `Lexicons` and saves all the words on these lists in a dictionary named `lexicons`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from os.path import join\n",
    "import re\n",
    "\n",
    "lexicons = dict()\n",
    "\n",
    "dir = 'Lexicons'\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "\n",
    "    topic = file[ : file.rindex('.') ]\n",
    "    words = []\n",
    "    \n",
    "    with open( join(dir,file) , encoding = 'utf-8' ) as file_handler:   \n",
    "        for l in file_handler: \n",
    "            if re.search( r'\\w' , l ):\n",
    "                words.append(l.strip())\n",
    "\n",
    "    lexicons[topic] = words    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the code below to count the number of occurrences of the words in these various lexicons within the texts of your corpus. The code searches in lemmatised versions of all the corpus texts. The result (consisting of counts for all the texts in your corpus) is stored in a file named 'lexicon.csv'.\n",
    "\n",
    "If your texts are long, or if the corpus contains many texts, running the code make take quite a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize,sent_tokenize,pos_tag\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from text_mining import *\n",
    "\n",
    "csv = open( 'lexicon.csv' , 'w' , encoding = 'utf-8' )\n",
    "\n",
    "## print header\n",
    "csv.write( 'lexicon,relative_count\\n' )\n",
    "\n",
    "print( f'\\nLemmatising {file_name} ...') \n",
    "path = file_name\n",
    "with open( path , encoding = 'utf-8' ) as fh:\n",
    "    full_text = fh.read()\n",
    "lemmatised = lemmatise(full_text)\n",
    "\n",
    "print( 'Performing semantic tagging for {} ...'.format( file_name ) )\n",
    "\n",
    "words = word_tokenize(lemmatised)\n",
    "words = remove_punctuation(words)\n",
    "freq = dict()\n",
    "for w in words:\n",
    "    freq[w] = freq.get(w,0)+1\n",
    "tokens = len(lemmatised)\n",
    "\n",
    "for l in lexicons:\n",
    "    print(f'{l} ...')    \n",
    "\n",
    "    countOccurrences = 0\n",
    "    for word in l:\n",
    "        countOccurrences += freq.get(word,0)\n",
    "\n",
    "    csv.write( f'{l},{countOccurrences / tokens}\\n')\n",
    "\n",
    "\n",
    "csv.close()\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, the counts that have made for the terms from the various lexicons can be visualised as a bar chart. As the value of the variable named `y`, you need to type in the name of the lexicon, without the .txt extension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('lexicon.csv')\n",
    "\n",
    "fig = plt.figure( figsize=( 7 ,6 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "x = 'lexicon'\n",
    "y = 'relative_count'\n",
    "\n",
    "\n",
    "bar_width = 0.45\n",
    "opacity = 0.8\n",
    "\n",
    "ax.bar( df[x] , df[y] , width = bar_width, alpha = opacity , color = '#23a145')\n",
    "\n",
    "plt.xticks(rotation= 75)\n",
    "\n",
    "ax.set_xlabel('Categories' , fontsize= 12)\n",
    "ax.set_ylabel('Lexicons' , fontsize = 12 )\n",
    "ax.set_title( y.title() , fontsize=20 )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.1\n",
    "\n",
    "Using the lexicon 'Religion.txt', can you find all the terms in *Ullyses* that are related to religion? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = 'https://raw.githubusercontent.com/peterverhaar/textmining_with_python/refs/heads/main/Corpus/Ullyses.txt'\n",
    "download(my_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "file_name = basename(my_file)\n",
    "\n",
    "print( f'\\nLemmatising {file_name} ...') \n",
    "path = file_name\n",
    "with open( path , encoding = 'utf-8' ) as fh:\n",
    "    full_text = fh.read()\n",
    "lemmatised = lemmatise(full_text)\n",
    "\n",
    "print( 'Performing semantic tagging for {} ...'.format( file_name ) )\n",
    "\n",
    "words = word_tokenize(lemmatised.lower())\n",
    "words = remove_punctuation(words)\n",
    "\n",
    "words = word_tokenize(lemmatised.lower())\n",
    "words = remove_punctuation(words)\n",
    "\n",
    "found_words = []\n",
    "for word in words:\n",
    "    if word in lexicons['Religion']:\n",
    "        found_words.append(word)\n",
    "        \n",
    "freq = Counter(found_words)\n",
    "\n",
    "for word,count in freq.most_common(30):\n",
    "    print(f\"{word} => {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_words = []\n",
    "for word in words:\n",
    "    if word in lexicons['Weather']:\n",
    "        found_words.append(word)\n",
    "        \n",
    "freq = Counter(found_words)\n",
    "\n",
    "for word,count in freq.most_common(30):\n",
    "    print(f\"{word} => {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_words = []\n",
    "for word in words:\n",
    "    if word in lexicons['Military']:\n",
    "        found_words.append(word)\n",
    "        \n",
    "freq = Counter(found_words)\n",
    "\n",
    "for word,count in freq.most_common(30):\n",
    "    print(f\"{word} => {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
