{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = 'https://raw.githubusercontent.com/peterverhaar/textmining_with_python/refs/heads/main/Corpus/PrideAndPrejudice.txt'\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "import sys\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "import requests\n",
    "\n",
    "from os.path import basename\n",
    "\n",
    "def download(url):\n",
    "    response = requests.get(url)\n",
    "    if response:\n",
    "        file_name = basename(url)\n",
    "        print(f\"{file_name} is downloaded!\")\n",
    "        out = open(file_name,'w',encoding='utf-8')\n",
    "        out.write(response.text)\n",
    "        out.close()\n",
    "\n",
    "        \n",
    "def sorted_by_value( dict , ascending = True ):\n",
    "    if ascending: \n",
    "        return {k: v for k, v in sorted(dict.items(), key=lambda item: item[1])}\n",
    "    else:\n",
    "        return {k: v for k, v in reversed( sorted(dict.items(), key=lambda item: item[1]))}\n",
    "\n",
    "download('https://raw.githubusercontent.com/peterverhaar/textmining_with_python/refs/heads/main/text_mining.py')\n",
    "download(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Examining the context of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordances\n",
    "\n",
    "Lists of frequent words can be very useful: they can help to clarify the main concerns or the themes of a text. To examine *how* words are used in a text, it can be useful, additionally, to create a concordance. In a concordance, all the occurrences of a given search term are listed in combination with words that occur before and after this term. Such resources are sometimes referred to as *keyword in context* lists (KWIC). \n",
    "\n",
    "The `nltk` package contains a method named `concordance()`. To work with this method, you firstly need to create an instance of the `Text` class. This class is part of the `nltk.text` module. Such a `Text` object can be initialised using a list with all the tokens of a text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.text import Text\n",
    "import os\n",
    "\n",
    "with open( 'PrideAndPrejudice.txt' , encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "tokens = word_tokenize(full_text)\n",
    "novel = Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the `Text` object is given the name `novel`. \n",
    "\n",
    "Once you have created such an object, you can use its `concordance()` method. You can supply three parameters: \n",
    "\n",
    "1. A search term.\n",
    "2. A `width` parameter, specifying the extent of the context. With this parameter, you indicate the number of characters before and after the word whose context you want to see. \n",
    "3. A `lines` parameter, which specifies the number of results. \n",
    "\n",
    "Out of these parameters, only the first one is mandatory. When you leave out the last two parameters, the method will work with its default values: a width of 70 (35 characters before and 35 characters after the search term) and 25 lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel.concordance('marriage' , width = 50 , lines = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `concordance()` method that is defined in `nltk`, the width of the context is defined using a specific number of characters. When you work with such a fixed number of characters, the search term can be shown at the same position on each line, resulting in a keyword-in-context list with a nice and orderly appearance.\n",
    "\n",
    "The downside of this approach is that the various lines may contain incomplete words. Which you indicate that the size of the context must be set at 20 characters before and after the term, the code simply removes all characters preceding or following the twentieth character. \n",
    "\n",
    "The cell below contains a definition of a method which can create a somwhat different type of concordance. In this method, named `concordance_words()`, the width of the context is specified on the basis of words rather characters. When you supply the number 10 as the value for the parameter defining the width, you will receive all occurrences of the search term, together with 5 words before and 5 words after this search term. The method demands a sting as input. This string can be the full text of a novel. \n",
    "\n",
    "The results are returned as a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from text_mining import *\n",
    "\n",
    "def concordance_word( text, regex , width = 10 ):\n",
    "\n",
    "    concordance = []\n",
    "    distance = math.floor( width /2 )\n",
    "\n",
    "    segment_length = 0\n",
    "\n",
    "    words = word_tokenize( text )\n",
    "    words = remove_punctuation( words )\n",
    "    i = 0\n",
    "    for w in words:\n",
    "        if re.search( regex , w , re.IGNORECASE ):\n",
    "            match = ''\n",
    "            for x in range( i - distance , ( i + distance ) + 1 ):\n",
    "                if x >= 0 and x < len(words):\n",
    "                    if len(words[x]) >= 0:\n",
    "                        match += words[x] + ' '\n",
    "            concordance.append( match )\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return concordance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains an illustration of how you can use this method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'PrideAndPrejudice.txt'\n",
    "\n",
    "with open( path, encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "    \n",
    "fragments = concordance_word( full_text , r'marriage' , 16)\n",
    "\n",
    "print( f'There are {len(fragments)} ocurrences of the word \"marriage\".')\n",
    "\n",
    "number_of_results = 5\n",
    "\n",
    "print( f'Here are the first {number_of_results} occurrences:\\n\\n')\n",
    "for f in fragments[:number_of_results]:\n",
    "    print( f'{f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the definition of `concordance_word()`, the method searches for occurrences of the supplied search term as a [regular expression](https://cdsleiden.github.io/python-tutorial/notebooks/9%20Regular_expressions.html). The second parameter of this method can also be a more complicated regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = concordance_word(full_text , r'(\\bhates?\\b)|(\\bloves?\\b)' , 25)\n",
    "\n",
    "for f in fragments[15:22]:\n",
    "    print( f'{f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation analysis\n",
    "\n",
    "Collocation analyses focus on the words that are used in the vicinity of a provided search term. It may be viewed as an extension of the principle underlying the creation of concordances. To perform a collocation analysis, we can look at the environments of a search term through a 'window' consisting of a given number of words. The words that are used in this context can obviously be counted. The aim of a collocation analysis is to identify the words that are used most frequently in the neighbourhood of a given word. \n",
    "\n",
    "Such collocation analyses can be carried out using the `collocation()` method that is defined below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocation( text , regex , width ):\n",
    "\n",
    "    freq_c = dict()\n",
    "    distance = math.floor(width/2)\n",
    "\n",
    "    sentences = sent_tokenize( text )\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        words = word_tokenize( sentence )\n",
    "        words = remove_punctuation(words)\n",
    "\n",
    "        for i,w in enumerate(words):\n",
    "            if re.search( regex , w , re.IGNORECASE ):\n",
    "                index_regex = i \n",
    "\n",
    "                for x in range( i - distance , i + distance ):\n",
    "                    if x >= 0 and x < len(words) and words[x].lower() != words[index_regex].lower():\n",
    "                        if len(words[x]) > 0:\n",
    "                            word = words[x].lower()\n",
    "                            freq_c[ word ] = freq_c.get( word , 0 ) + 1\n",
    "            \n",
    "    return freq_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are the same as those of the `concordance_word()` method: \n",
    "\n",
    "1. The text that needs to be analysed.\n",
    "2. A search term, which will be treated as a regular expression.\n",
    "3. A number representing the width of the context (or, ot be more precise: the number of words). \n",
    "\n",
    "This function returns a dictionary listing all the words found near the search term that is provided, together with the frequencies of these words.  \n",
    "\n",
    "The code below makes use of the function `sortedByValue()` which can sort a dictionary by value, and the list of stopwords from `nltk` to remove the function words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortedByValue( dict , ascending = True ):\n",
    "    if ascending: \n",
    "        return {k: v for k, v in sorted(dict.items(), key=lambda item: item[1])}\n",
    "    else:\n",
    "        return {k: v for k, v in reversed( sorted(dict.items(), key=lambda item: item[1]))}\n",
    "\n",
    "\n",
    "nearby_words = collocation( full_text , r'marriage' , 20)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "nearby_words_sorted = sortedByValue( nearby_words , ascending = False)\n",
    "\n",
    "for word in list( nearby_words_sorted.keys() ):\n",
    "    freq = nearby_words_sorted[word]\n",
    "    if word not in stopwords and freq > 2:\n",
    "        print( f'{word} => {freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooccurrence\n",
    "\n",
    "Once you have established that two specific words are often used in combination, you can begin to study specific combinations of words in more detail using the `cooccurrence()` method that is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence( text , word1 , word2 , width ):\n",
    "    \n",
    "    relevant_sentences = []\n",
    "    \n",
    "    text = re.sub( '\\s+' , ' ' , text )\n",
    "    sentences = sent_tokenize( text )\n",
    "\n",
    "    for s in sentences:\n",
    "        if re.search( r'\\b' + word1 + r'\\b' , s , re.IGNORECASE ) and re.search( r'\\b' + word2 + r'\\b' , s , re.IGNORECASE ):\n",
    "\n",
    "            words = word_tokenize(s)\n",
    "            word1_indexes = []\n",
    "            word2_indexes = []\n",
    "            \n",
    "            for i,w in enumerate(words):\n",
    "                if re.search( r'\\b' + word1 + r'\\b' , w , re.IGNORECASE ):\n",
    "                    word1_indexes.append(i)\n",
    "                elif re.search( r'\\b' + word2 + r'\\b' , w , re.IGNORECASE ):\n",
    "                    word2_indexes.append(i)\n",
    "\n",
    "            if word1_indexes[0] > word2_indexes[0]:\n",
    "                difference = word1_indexes[0] - word2_indexes[0]\n",
    "            else:\n",
    "                difference = word2_indexes[0] - word1_indexes[0]\n",
    "\n",
    "            if difference <= width:\n",
    "                relevant_sentences.append(s)\n",
    "    return relevant_sentences\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The useage of the method is as follows:\n",
    "    \n",
    "* As the first parameter, you mus provide the full text that you want to analyse, as a single string.\n",
    "* As the second and the third parameter, you need to mention the two words that you are interested in. \n",
    "* How close should these two words be? The fourth parameter specifies the number of words that are allowed in between the two words you focus on.  \n",
    "\n",
    "The method `cooccurrence()` returns all the sentences containing the two words that you focus on. The distance (measured in number of words) will not be greater than the width that you specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = cooccurrence( full_text , 'marriage' , 'lydia' , 10 )\n",
    "\n",
    "for s in sentences:\n",
    "    print( f'{s}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.1\n",
    "\n",
    "Create a concordance for the word 'savage' in the novel *Brave New World*. You can find the full text in the 'Corpus' folder. Work with a width of 50 characters (i.e. 25 characters before and 25 characters after this search term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.2. \n",
    "\n",
    "Create a concordance for the word 'soma' in the novel *Brave New World*. This time, work with a width of 20 words (i.e. 10 words before and 10 words after this search term). Display the first 15 occurrences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.3\n",
    "\n",
    "In *Ullyses*, which words are used most frequently in the vicinity of the word 'father'? Consider 8 words before and 8 words after all the occurrences of this specific name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.4\n",
    "\n",
    "Find all the sentences in *Ullyses* that contain the words 'book' and 'read'. Make sure that, in these sentences, there are no more than 10 words in between these two words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
