{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Type-token ratio\n",
    "\n",
    "As was discussed in one of the previous notebooks, the individual words that are found in a text are referred to as “tokens”, and the unique words are called “types”. Frequency lists count occurrences of types. \n",
    "\n",
    "The ratio between the number of types and the number of tokens can, under certain conditions, offer useful information about texts as well. The type-token ratio is calculated by dividing the number of types by the number of tokens. This division obviously results a number in between 0 and 1. This number gives an indication of the lexical diversity: the capacity of the author to vary the vocabulary. \n",
    "\n",
    "If the type-token ratio is high, this indicates that the author uses many unique words and that the text contains very little lexical repetition. If, by contrast, the type-token ratio is low, this implies that there is low level of lexical diversity; the same words are over and over. \n",
    "\n",
    "The type-token ratio can be calculated using the `word_tokenize()` function from the `nltk` package, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from tdmh import *\n",
    "\n",
    "dir = 'Corpus'\n",
    "text = 'ARoomWithAView.txt'\n",
    "path = join( dir, text )\n",
    "\n",
    "with open( path , encoding = 'utf-8' ) as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "words = tokenise(full_text)\n",
    "\n",
    "tokens = len(words)\n",
    "unique_words = set(words)\n",
    "types = len(unique_words)\n",
    "\n",
    "ttr = types / tokens\n",
    "\n",
    "print( f'Types: {types}' )\n",
    "print( f'Tokens: {tokens}' )\n",
    "print( f'Type-token ratio: {ttr}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above makes use of the function `set()`. It can be used to convert a Python list into a set. A set is default data structure in Python, very similar to a list. An important difference, however, is that, while a list may contain the same item multiple times, a set can only contain unique items. A list also stores the items in a specific order, while a set is **unordered**. The `set()` function can be used very effectively to deduplicate a Python list.\n",
    "\n",
    "## Comparing the lexical diversity of different texts\n",
    "\n",
    "The cell below defines a function named `ttr`. It contains the code that was explained to calculate the type-token ratio. This function only needs some text as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttr(full_text):\n",
    "    words = tokenise(full_text)\n",
    "    words = remove_punctuation(words)\n",
    "\n",
    "    tokens = len(words)\n",
    "    unique_words = set(words)\n",
    "    types = len(unique_words)\n",
    "\n",
    "\n",
    "    return types / tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having this code wrapped together into a single funcion, the formula for calculating the type-token ratio can easily be applied to all the texts in a given corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from os.path import join\n",
    "import tdm\n",
    "\n",
    "    \n",
    "dir = 'Corpus'    \n",
    "for text in os.listdir(dir):\n",
    "    if re.search( r'\\.txt' , text ):\n",
    "        path = join( dir , text) \n",
    "        with open( path , encoding = 'utf-8' ) as file:\n",
    "            full_text = file.read()\n",
    "            full_text = full_text.lower()\n",
    "        \n",
    "        print( f'{ remove_extension(text) }: {ttr(full_text)} ' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you work with type-token ratios, it is important to realise that the result of such calculations tend to vary along with the total length of the text. In a relatively short text, it is easier for an author to continue to introduce new words as the text progresses. When texts become much longer, however, the chances that words will be repeated also increase accordingly. Shorter texts generally have much higher type-token ratios. \n",
    "\n",
    "One solution can be to ensure that all the texts are of the same lengths before calculating the type token ratios. We can do this by firstly calculating the length (i.e. the total number of words) of the **shortest text in the corpus**. Next, we can artifically harmonise the lengths of all the texts by creating substrings of the longer texts. These substrings should have exactly the same number of words as the shortest text in the corpus. The code below illustrates this principle.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'Corpus'\n",
    "texts = []\n",
    "min_tokens = 0 \n",
    "import tdmh\n",
    "\n",
    "for text in os.listdir(dir):\n",
    "    if re.search( r'\\.txt' , text ):\n",
    "        texts.append(text)\n",
    "        path = join( dir , text) \n",
    "        with open( path , encoding = 'utf-8' ) as file:\n",
    "            full_text = file.read()\n",
    "            words = tdmh.tokenise(full_text)\n",
    "            tokens = len(words)\n",
    "            print( f'{text} contains {tokens} words.' )\n",
    "            if min_tokens == 0:\n",
    "                min_tokens = tokens\n",
    "            elif tokens < min_tokens:\n",
    "                min_tokens = tokens\n",
    "                \n",
    "print( f'\\nShortest text has {min_tokens} words.\\n' )\n",
    "\n",
    "ttr_scores = dict()\n",
    "                \n",
    "            \n",
    "for text in texts:\n",
    "    if re.search( r'\\.txt' , text ):\n",
    "        path = join( dir , text) \n",
    "        print( f'Calculating the TTR of {path}' )\n",
    "        with open( path , encoding = 'utf-8' ) as file:\n",
    "            full_text = file.read()\n",
    "            full_text = full_text.lower()\n",
    "            full_text = full_text[ 0 : min_tokens]\n",
    "        \n",
    "        print( f'{ removeExtension(text) }: {ttr(full_text)} ' )\n",
    "        ttr_scores[ removeExtension(text) ] = ttr(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Exercise 5.1\n",
    "\n",
    "Try to create a bar chart which visualises the type-token ratios of all the texts in the folder 'Corpus'. \n",
    "\n",
    "As you do this, you can make use of the dictionary named `ttr_scores`, which is created in the last cell under the section 'Comparing the lexical diversity of different texts'. The titles of the texts in the corpus serve as keys, and the type-token ratios that are calculated are stored as the values. \n",
    "\n",
    "You can plot all the values in `list(ttr_scores.keys())` on the X-axis, and the values in `list(ttr_scores.values())` on the Y-axis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.2\n",
    "\n",
    "Calculate the type-token ratios of *Sons and Lovers*, *Ivanhoe* and *Through the Looking Glass*. Use the function `ttr()` that is defined in this notebook, but in this case, focus on the first 3000 words of the novels that are mentioned only.\n",
    "\n",
    "Does this result in different type-topken ratios? If yes, do these different numbers also prompt different conclusions about lexical diversity of these novels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
