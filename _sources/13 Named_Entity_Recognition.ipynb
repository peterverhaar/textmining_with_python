{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Named Entity Recognition\n",
    "\n",
    "\n",
    "A named entity is an object, person, location or organization which has been assigned a proper name. *Named Entity Recognition* is a computational technique that seeks to identify all the named entities that that are mentioned within texts. Applications making use of Named Entity Recognition can generally extract the most of the occurrences of such named entities and it can also characterise such entities using pre-defined categories such as ‘Person’, ‘Location’, ‘Work of Art’, ‘Organisation’. \n",
    "\n",
    "Named Entity Recognition applications typically make use of statistical models created using Machine Learning algorithms. Such models are often trained using large numbers of texts in which all the people, locations, organisations and named objects have been labelled manually by human readers. On the basis of a careful analysis of the frequencies and the contexts of all of these named entitities, computers can eventually be enabled to recognise similar types of entitities in new, unlabelled texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains how you can work with Named Entity Recognition using the `stanza`. This package includes [a range of state-of-the-art NLP models](https://stanfordnlp.github.io/stanza/available_models.html) which enable you to carry out tasks such as *part of speech tagging*, *sentiment analysis* and *named entity recognition*. \n",
    "\n",
    "The package can be installed as follows. After the installation, you probably need to restart the kernel before you can make use of the package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a successful installation, you should be able to import the package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to create an NLP pipeline. As you you do this, you need to specify the language of the texts in your corpus, together with the type of NLP models you want to work with. In the cell below, the pipeline is given the name `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 11:36:40 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2024-10-30 11:36:40 INFO: Use device: cpu\n",
      "2024-10-30 11:36:40 INFO: Loading: tokenize\n",
      "2024-10-30 11:36:40 INFO: Loading: ner\n",
      "2024-10-30 11:36:40 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline object can then be used to parse natural language texts. If the 'ner' model is selected, the pipeline will create a variable named `ents`, containing all the named entities that were found in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James Joyce\ttype: PERSON\n",
      "February 2, 1882\ttype: DATE\n",
      "Dublin\ttype: GPE\n",
      "Ireland\ttype: GPE\n",
      "January 13, 1941\ttype: DATE\n",
      "Zürich\ttype: GPE\n",
      "Switzerland\ttype: GPE\n",
      "Irish\ttype: NORP\n",
      "Ulysses (1922)\ttype: WORK_OF_ART\n",
      "Finnegans Wake\ttype: WORK_OF_ART\n",
      "1939\ttype: DATE\n"
     ]
    }
   ],
   "source": [
    "sentence = \"James Joyce (born February 2, 1882, Dublin, Ireland - died January 13, 1941, Zürich, Switzerland) was Irish novelist noted for his experimental use of language and exploration of new literary methods in such large works of fiction as Ulysses (1922) and Finnegans Wake (1939).\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for named_entity in doc.ents:\n",
    "    print(f\"{named_entity.text}\\ttype: {named_entity.type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stanza` works with the following pre-defined NER labels: \n",
    "\n",
    "* PERSON: People, including fictional \n",
    "* NORP: Nationalities or religious or political groups \n",
    "* FAC: Buildings, airports, highways, bridges, etc. \n",
    "* ORG: Companies, agencies, institutions, etc. \n",
    "* GPE: Countries, cities, states \n",
    "* LOC: Non-GPE locations, mountain ranges, bodies of water \n",
    "* PRODUCT: Objects, vehicles, foods, etc. (not services) \n",
    "* EVENT: Named hurricanes, battles, wars, sports events, etc. \n",
    "* WORK_OF_ART: Titles of books, songs, etc. \n",
    "* LAW: Named documents made into laws. \n",
    "* LANGUAGE: Any named language \n",
    "* DATE: Absolute or relative dates or periods \n",
    "* TIME: Times smaller than a day \n",
    "* PERCENT: Percentage, including \"%\" \n",
    "* MONEY: Monetary values, including unit \n",
    "* QUANTITY: Measurements, as of weight or distance \n",
    "* ORDINAL: \"first\", \"second\", etc. \n",
    "* CARDINAL: Numerals that do not fall under another type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding NER tags in longer texts\n",
    "\n",
    "When you work with `stanza`, it is usefl to bear in mind that its parser roughly requires 1GB of memory per 100,000 characters. Texts containing more than 1,000,000 characters tends to cause memory allocation errors. \n",
    "\n",
    "The code below tries to avoid such errors. The code divides the full text into segments, each of which are shorter than a maximum named `max_length`. The  `max_length` of the texts to be parsed has been set safely to 100,000. \n",
    "\n",
    "After this, these shorter segments are all parsed one by one. These tagged texts are stored in a dictionary named `tagged_segments`. \n",
    "\n",
    "Tagging texts of ca. 100,000 characters still demands quite some memory space. The code below may take some time to complete because of this.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in PrideAndPrejudice.txt: 685008\n",
      "The text has been divided into 7 segments.\n",
      "Annotating the text segments ... \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "segments = dict()\n",
    "tagged_segments = dict()\n",
    "segment_nr = 0\n",
    "\n",
    "text = 'PrideAndPrejudice.txt'\n",
    "dir = 'Corpus'\n",
    "path = join( dir, text )\n",
    "max_length = 100000\n",
    "\n",
    "with open(path, encoding = 'utf-8') as file_handler:\n",
    "    full_text = file_handler.read()\n",
    "    \n",
    "print( f'Total number of characters in {text}: {len(full_text)}' )\n",
    "\n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "length = 0 \n",
    "segment = ''\n",
    "\n",
    "for s in sentences:\n",
    "    length += len(s)\n",
    "    if length < max_length:\n",
    "        segment += s + ' '\n",
    "    else:\n",
    "        segments[segment_nr] = segment\n",
    "        segment = s + ' '\n",
    "        length = 0 \n",
    "        segment_nr += 1\n",
    "        \n",
    "if len(segment) > 0:\n",
    "    segments[segment_nr] = segment\n",
    "    \n",
    "print(f\"The text has been divided into {len(segments)} segments.\")\n",
    "    \n",
    "print( 'Annotating the text segments ... ')    \n",
    "for i in segments:\n",
    "    print(i)\n",
    "    tagged_segments[i] = nlp(segments[i])\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotated texts can be analysed in a variety of ways. The code below lists the personal names that are mentioned most frequently in the text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm import sortedByValue\n",
    "\n",
    "freq = dict()\n",
    "\n",
    "for doc in tagged_segments:\n",
    "    for named_entity in tagged_segments[doc].ents:\n",
    "        if named_entity.label_ == 'PERSON':\n",
    "            name = str(named_entity) \n",
    "            name = name.strip()\n",
    "            freq[ name ] = freq.get( name , 0 ) + 1\n",
    "        \n",
    "for name in reversed( sortedByValue(freq) ):\n",
    "    print( f'{name}: {freq[name]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view all the works of art that are referred to in the texts, for instance, you need to replace the label 'PERSON' in the code below with the tag 'WORK_OF_ART'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.1\n",
    "\n",
    "Using the code that is explained in this notebook, try to find all the location that are mentioned in *Pride and Prejudice*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
