{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Quantitative analyses of texts are often based initially on counts of the smaller linguistic units that occur within texts, such as the words, the paragraph, or the sentences. The process of dividing a text into such smaller components is referred to as tokenisation. \n",
    "\n",
    "Word tokenisation generally takes place on the basis of the spaces that occur in between the words. The individual words that are found in a text are called “**tokens**\". When this full list is deduplicated, leaving only the unique words, the items in such lists are called “**types**”. \n",
    "\n",
    "This tutorial explains how you can tokenise a text using the `nltk` package, a toolkit that enables you you work with texts in natural languages. \n",
    "\n",
    "To make use of this package, you firstly need to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/verhaarpaf/Library/Python/3.12/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/verhaarpaf/Library/Python/3.12/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/verhaarpaf/Library/Python/3.12/lib/python/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (4.66.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods that are discussed in this tutorial make use of a number of additional resources which are not installed by default. If you have never used `nltk` before, you need to run the code below to install these resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/verhaarpaf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/verhaarpaf/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/verhaarpaf/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/verhaarpaf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/verhaarpaf/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenisation \n",
    "\n",
    "The `ntlk.tokenize` module has a method named `word_tokenize()`. This methods demand a string as input. This may be a sentence or a whole paragraph. When it is run, the method returns a Python list containing all the individual words found in the string that is provided. As mentioned, the individual words are identified using the spaces found in this string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'best', 'of', 'times', ',', 'it', 'was', 'the', 'worst', 'of', 'times']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "quote = '''It was the best of times, \n",
    "it was the worst of times'''\n",
    "\n",
    "words = word_tokenize(quote)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look closely at the output of the code above, you can see that the `word_tokenize()` method treats punctuation marks as separate tokens. In the example above, the comma following the first occurrence of the word 'times' is actually a separate item in the list. \n",
    "\n",
    "The function `remove_punctuation`, defined below, can be used to remove the tokens that consist of punctuation only. As input, the function demands a list of tokens. For each string in this list, the function tests whether it consists of alphanumeric characters, using the `isalnum()` method. The function returns those words which pass this test only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        if w.isalnum():\n",
    "            new_list.append( w )\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times']\n"
     ]
    }
   ],
   "source": [
    "words = remove_punctuation(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created a list containing all the individual tokens, you can easily count the total number of token, using the `len()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text fragment contains 12 tokens.\n"
     ]
    }
   ],
   "source": [
    "nr_tokens = len( words )\n",
    "\n",
    "print( f'The text fragment contains {nr_tokens} tokens.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence tokenisation\n",
    "\n",
    "The method `sent_tokenize()` from the `nltk` package can be used to divide a text into its separate sentences. This type of tokenisation take place on the basis of full stops and upper case characters. \n",
    "\n",
    "The cell below contains an illustration of how the `sent_tokenize()` method can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fragment contains 4 sentences.\n",
      "\n",
      "In the late summer of that year we lived in a house in a village that looked across the river and the plain to the mountains.\n",
      "\n",
      "In the bed of the river there were pebbles and boulders, dry and white in the sun, and the water was clear and swiftly moving and blue in the channels.\n",
      "\n",
      "Troops went by the house and down the road and the dust they raised powdered the leaves of the trees.\n",
      "\n",
      "The trunks of the trees too were dusty and the leaves fell early that year and we saw the troops marching along the road and the dust rising and leaves, stirred by the breeze, falling and the soldiers marching and afterward the road bare and white except for the leaves.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "## First paragraph of \"A Farewell to Arms\"\n",
    "quote = '''In the late summer of that year we lived in a house in a village that looked across the river and the plain to the mountains. In the bed of the river there were pebbles and boulders, dry and white in the sun, and the water was clear and swiftly moving and blue in the channels. Troops went by the house and down the road and the dust they raised powdered the leaves of the trees. The trunks of the trees too were dusty and the leaves fell early that year and we saw the troops marching along the road and the dust rising and leaves, stirred by the breeze, falling and the soldiers marching and afterward the road bare and white except for the leaves.'''\n",
    "\n",
    "sentences = sent_tokenize(quote)\n",
    "\n",
    "print( f'The fragment contains { len(sentences) } sentences.\\n' )\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.1.\n",
    "\n",
    "The 'Corpus' that you have downloaded as part of this tutorial contains the full text of 'Pride and Prejudice'. \n",
    "Write code that can help you to answer the following questions about this text:\n",
    "\n",
    "* How many characters are there in the novel?\n",
    "* How many words does the novel contain?\n",
    "* How often does the novel mention the word \"marriage\"?\n",
    "* How long are the words in *Pride and Prejudice* on average (measured in number of characters)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120081\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "text_file = open('Corpus/PrideAndPrejudice.txt',encoding='utf-8')\n",
    "full_text = text_file.read()\n",
    "\n",
    "words = nltk.word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "\n",
    "print(len(words))\n",
    "\n",
    "print(words.count('marriage'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to => 4076\n",
      "the => 4056\n",
      "of => 3605\n",
      "and => 3371\n",
      "her => 2131\n",
      "I => 2020\n",
      "a => 1874\n",
      "was => 1842\n",
      "in => 1783\n",
      "that => 1520\n",
      "not => 1506\n",
      "she => 1378\n",
      "it => 1273\n",
      "be => 1230\n",
      "his => 1191\n",
      "had => 1148\n",
      "you => 1137\n",
      "as => 1132\n",
      "he => 1096\n",
      "for => 1029\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "frequencies = Counter(words)\n",
    "\n",
    "for word,count in frequencies.most_common(20):\n",
    "    print(f\"{word} => {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.2.\n",
    "\n",
    "Can you print the first five sentences of *Pride and Prejudice*?\n",
    "How many sentences does the novel contain in total?\n",
    "Building on the results of the previous exercise, can you calculate the average length of sentences? In other words, how many tokens do the sentences contain on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.3.\n",
    "\n",
    "What is the longest word in the novel *Ullyses*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
