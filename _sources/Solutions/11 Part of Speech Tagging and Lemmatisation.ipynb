{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Part of Speech Tagging and Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.1\n",
    "\n",
    "Create a list containing the unique adjectives that are occur in *Pride and Prejudice*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "from collections import Counter\n",
    "\n",
    "path = os.path.join('..', 'Corpus','PrideAndPrejudice.txt')\n",
    "\n",
    "with open( path , encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "words = word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "pos = pos_tag(words)\n",
    "\n",
    "adjectives = []\n",
    "adj_codes = ['JJ','JJR','JJS']\n",
    "\n",
    "for p in pos:\n",
    "    if p[1] in adj_codes:\n",
    "        adjectives.append(p[0])\n",
    "        \n",
    "freq = Counter(adjectives)\n",
    "\n",
    "for word,count in freq.most_common(20):\n",
    "    print(f'{word} => {count}')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.2\n",
    "\n",
    "Stephen King is [reputed to have said](https://www.goodreads.com/quotes/430289-i-believe-the-road-to-hell-is-paved-with-adverbs) that â€œthe road to hell is paved with adverbs\", and many style guides similarly give writers the advice to avoid adverbs, especially those ending in '-ly'. \n",
    "\n",
    "Can you calculate, for each text in the corpus, the number of adverb ending in '-ly', measured as a percentage of the total number of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "from collections import Counter\n",
    "\n",
    "directory = os.path.join('..','Corpus')\n",
    "files = os.listdir(directory)\n",
    "\n",
    "for file in files:\n",
    "    print(f\"\\n{file}\")\n",
    "    path = os.path.join(directory,file)\n",
    "    \n",
    "    full_text = ''\n",
    "    with open( path , encoding = 'utf-8') as file:\n",
    "        full_text = file.read()\n",
    "\n",
    "    words = word_tokenize(full_text.lower())\n",
    "    words = remove_punctuation(words)\n",
    "    nr_words = len(words)\n",
    "    pos = pos_tag(words)\n",
    "\n",
    "    adjectives = []\n",
    "    adj_codes = ['RB','RBR','RBS']\n",
    "\n",
    "    ly_adverbs = 0\n",
    "    for p in pos:\n",
    "        if p[1] in adj_codes and p[0][-2:].strip() == 'ly':\n",
    "            adjectives.append(p[0])\n",
    "            ly_adverbs += 1\n",
    "\n",
    "    freq = Counter(adjectives)\n",
    "        \n",
    "    print(f\"{ly_adverbs} adverbs ending in '-ly' in total.\")\n",
    "    print(f\"This is {round(ly_adverbs/nr_words,4)}% of all the words \")\n",
    "    \n",
    "    number = 15\n",
    "    if ly_adverbs>0:\n",
    "        print(f\"{number} most frequent adverbs:\")\n",
    "        for word,count in freq.most_common(number):\n",
    "            print(f'{word} => {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.3\n",
    "\n",
    "Which text in the corpus has the highest number of modal verbs? The Penn Treebank code for 'modal auxialiaries' is MD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "from collections import Counter\n",
    "\n",
    "directory = os.path.join('..','Corpus')\n",
    "files = os.listdir(directory)\n",
    "\n",
    "for file in files:\n",
    "    print(f\"\\n{file}\")\n",
    "    path = os.path.join(directory,file)\n",
    "    \n",
    "    full_text = ''\n",
    "    with open( path , encoding = 'utf-8') as file:\n",
    "        full_text = file.read()\n",
    "\n",
    "    words = word_tokenize(full_text.lower())\n",
    "    words = remove_punctuation(words)\n",
    "    nr_words = len(words)\n",
    "    pos = pos_tag(words)\n",
    "    \n",
    "    modal_verbs = []\n",
    "\n",
    "    for p in pos:\n",
    "        if p[1] == 'MD' and len(p[0])>2:\n",
    "            modal_verbs.append(p[0])\n",
    "\n",
    "    freq = Counter(modal_verbs)\n",
    "        \n",
    "    print(f\"{len(modal_verbs)} modal verbs.\")\n",
    "\n",
    "    number = 10\n",
    "    if len(modal_verbs)>0:\n",
    "        print(f\"{number} most frequent modal verbs:\")\n",
    "        for word,count in freq.most_common(number):\n",
    "            print(f'{word} => {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.4\n",
    "\n",
    "Extract all the sentences from *BraveNewWorld.txt* that contain an adjective in the superlative form.  Write these sentences into a file named 'sentences.txt'. The code for the words in these category is 'JJS'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "from collections import Counter\n",
    "\n",
    "path = os.path.join('..', 'Corpus','BraveNewWorld.txt')\n",
    "\n",
    "with open( path , encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    words = remove_punctuation(words)\n",
    "    pos = pos_tag(words)\n",
    "    \n",
    "    adj = []\n",
    "    for p in pos:\n",
    "        if p[1] == 'JJS':\n",
    "            adj.append(p[0])\n",
    "            \n",
    "    if len(adj)>0:\n",
    "        print(f\"{sentence} [{'|'.join(adj)}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise11.5\n",
    "\n",
    "Extract all the sentences from *Ullyses.txt* containing a form of the verb 'to see', in all tenses and conjugations and excepting the infitive form. In other words, extract sentences containing forms such as 'seen', 'saw' or 'seeing', but not 'see'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from text_mining import *\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "path = os.path.join('..', 'Corpus','Ullyses.txt')\n",
    "\n",
    "full_text = ''\n",
    "with open( path , encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "for sentence in sentences:\n",
    "\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    words = remove_punctuation(words)\n",
    "\n",
    "    pos = nltk.pos_tag(words)\n",
    "    \n",
    "    hits = []\n",
    "\n",
    "    for i,word in enumerate(words):\n",
    "        word = word.lower()\n",
    "        posTag = ptb_to_wordnet( pos[i][1] )\n",
    "\n",
    "        if re.search( r'\\w+' , posTag , re.IGNORECASE ):\n",
    "            lemma = lemmatiser.lemmatize( words[i] , posTag )\n",
    "            if lemma == 'see':\n",
    "                hits.append(word)\n",
    "        else:\n",
    "            if word == 'see':\n",
    "                hits.append(word)\n",
    "                \n",
    "    if len(hits)>0:\n",
    "        print(f\"{sentence}\\n---\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.6\n",
    "\n",
    "From *Ullyses.txt*, extract all sentences containing the following combinations of categories: \n",
    "\n",
    "* Article - adverb - adjective - noun \n",
    "\n",
    "These categories can be assigned the following codes:\n",
    "\n",
    "* Article: DT\n",
    "* Adverb: RB, RBR or RBS\n",
    "* Adjective: JJ, JJR or JJS\n",
    "* Noun: NN, NNP, NNPS or NNS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize , sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tdm import *\n",
    "\n",
    "\n",
    "from os.path import join \n",
    "\n",
    "path = join('..', 'Corpus','Ullyses.txt' )\n",
    "with open( path , encoding = 'utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    \n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = re.sub(r'\\n',' ',sentence)\n",
    "    words = word_tokenize(sentence)\n",
    "    words = remove_punctuation(words)\n",
    "    pos = pos_tag(words)\n",
    "    \n",
    "    tagged_sentence = ''\n",
    "\n",
    "    for p in pos:\n",
    "        tagged_sentence += p[1] + ' '\n",
    "\n",
    "    if re.search( r'DT RB JJ NN' , tagged_sentence):\n",
    "        print(f\"{sentence}\\n---\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
